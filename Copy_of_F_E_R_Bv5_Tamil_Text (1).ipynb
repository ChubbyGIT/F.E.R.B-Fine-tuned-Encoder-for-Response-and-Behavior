{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVa0caPZlogN"
      },
      "source": [
        "# F.E.R.B – Fine-tuned Encoder for Response and Behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT5BjFcflZAh"
      },
      "source": [
        "## Installing Required Libraries for AI Model Deployment\n",
        "\n",
        "This command installs the necessary Python libraries for running and fine-tuning transformer-based AI models efficiently:\n",
        "\n",
        "accelerate (v0.21.0): Optimizes deep learning model training and inference for better performance.\n",
        "peft (v0.4.0): Enables efficient fine-tuning of large models using techniques like LoRA (Low-Rank Adaptation).\n",
        "bitsandbytes (v0.40.2): Supports 8-bit and 4-bit quantization, reducing memory usage for large models.\n",
        "transformers (v4.31.0): Provides pre-trained NLP models like LLaMA, GPT, and BERT from Hugging Face.\n",
        "trl (v0.4.7): Aids in reinforcement learning fine-tuning for transformer models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXixfzPMmuFx"
      },
      "source": [
        "##Installing Hugging Face Hub for Model Access\n",
        "\n",
        "This command installs the Hugging Face Hub library, which allows seamless access to pre-trained models, datasets, and tokenizers from Hugging Face. It enables functionalities such as:\n",
        "\n",
        "Downloading and using models from the Hugging Face Model Hub.\n",
        "Uploading and sharing custom models and datasets.\n",
        "Managing authentication and API tokens for secure access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRZm_OAbs3qA"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5CIZ1UJnFSe"
      },
      "source": [
        "##Upgrading Key Libraries for Efficient Model Execution\n",
        "\n",
        "Upgrades BitsandBytes, which enables 8-bit and 4-bit quantization, reducing memory usage for large transformer models.\n",
        "Upgrades Transformers, the core library for working with pre-trained NLP models like LLaMA, GPT, and BERT.\n",
        "Upgrades Accelerate, which optimizes multi-GPU and mixed-precision training for faster and more efficient deep learning model execution.\n",
        "TRL (Transformer Reinforcement Learning): Supports fine-tuning models with reinforcement learning techniques.\n",
        "PEFT (Parameter Efficient Fine-Tuning): Enables low-rank adaptation (LoRA) and other efficient fine-tuning methods.\n",
        "Datasets: Provides access to large-scale NLP datasets with easy preprocessing capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qef-eKbivyHW"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade bitsandbytes\n",
        "!pip install --upgrade transformers accelerate\n",
        "!pip install --upgrade trl peft datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDqKx4p0nST-"
      },
      "source": [
        "##Checking NVIDIA CUDA Compiler Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atYf_-uRxJOb"
      },
      "outputs": [],
      "source": [
        "!nvcc --version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0i4M6g4nal6"
      },
      "source": [
        "## Importing Libraries for Fine-Tuning LLMs\n",
        "\n",
        "Torch: PyTorch for deep learning.\n",
        "TRL & SFTTrainer: Fine-tuning transformer models with reinforcement learning.\n",
        "PEFT & LoraConfig: Efficient parameter fine-tuning using LoRA.\n",
        "Datasets: Loading NLP datasets.\n",
        "Transformers (AutoModelForCausalLM, AutoTokenizer, etc.): Handling LLMs like LLaMA and GPT.\n",
        "BitsAndBytesConfig: Enables 8-bit quantization for memory efficiency.\n",
        "Pipeline: Simplifies text generation and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz3vMSzhs-P7"
      },
      "source": [
        "## Loading and Quantizing a Fine-Tuned LLaMA Model\n",
        "\n",
        "AutoModelForCausalLM.from_pretrained(...) loads the model from \"aboonaji/llama2finetune-v2\".\n",
        "\n",
        "BitsAndBytesConfig(load_in_4bit=True, ...) enables 4-bit quantization, reducing memory consumption.\n",
        "\n",
        "bnb_4bit_compute_dtype=torch.float16 ensures faster computations in float16 precision.\n",
        "\n",
        "bnb_4bit_quant_type=\"nf4\" improves numerical stability using NormalFloat4 (NF4) quantization.\n",
        "\n",
        "llama_model.config.use_cache = False disables caching to avoid memory issues during fine-tuning.\n",
        "\n",
        "llama_model.config.pretraining_tp = 1 sets tensor parallelism to 1, ensuring compatibility in single-GPU setups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQP8U95iiURD"
      },
      "outputs": [],
      "source": [
        "llama_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = \"aboonaji/llama2finetune-v2\",\n",
        "                                                   quantization_config = BitsAndBytesConfig(load_in_4bit = True,\n",
        "                                                                                            bnb_4bit_compute_dtype = getattr(torch, \"float16\"),\n",
        "                                                                                            bnb_4bit_quant_type = \"nf4\"))\n",
        "llama_model.config.use_cache = False\n",
        "llama_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6aWb1e7tNRS"
      },
      "source": [
        "## Loading and Configuring LLaMA Tokenizer\n",
        "\n",
        "This code loads the LLaMA 2 tokenizer from a fine-tuned model and configures it for proper text processing. It ensures that tokenization aligns with the model’s needs by setting a padding token and defining the padding side. This setup is essential for efficient text generation, fine-tuning, and inference, preventing tokenization-related issues during training and deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yboOSBFoXJcc"
      },
      "outputs": [],
      "source": [
        "llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = \"aboonaji/llama2finetune-v2\", trust_remote_code = True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coUlIR-ytjiF"
      },
      "source": [
        "##  Defining Training Parameters for Fine-Tuning\n",
        "\n",
        "This code sets up training configurations for fine-tuning a model. It specifies the output directory for saving results, the batch size per device during training, and the maximum number of training steps. These parameters help control the efficiency, memory usage, and overall training process for optimizing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB3RAAtQrp3I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ivh9ZCqSxdz"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(output_dir = \"./results\", per_device_train_batch_size = 4, max_steps = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaiDiDox0Kn9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c0Bma9Wy0kuS"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWx8G7ZKokp8"
      },
      "source": [
        "##Forcefully Clearing GPU Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MDe_d7J105Qz"
      },
      "outputs": [],
      "source": [
        "!kill -9 $(nvidia-smi | awk '$2==\"Processes:\" {f=1; next} f {print $5}' | xargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-XLuX0kor1Q"
      },
      "source": [
        "##Displaying GPU Status and Usage\n",
        "\n",
        "This command shows the GPU status, memory usage, temperature, and active processes using nvidia-smi (NVIDIA System Management Interface). It helps monitor GPU utilization, running processes, and potential memory bottlenecks, making it essential for deep learning and high-performance computing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HAfqtMga07ON"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V7kPBPWS1B3u"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RGLZtFwHQiZ"
      },
      "source": [
        "## Step 5: Fine-Tuning LLaMA 2 with LoRA and Optimized Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520SAVN5s1ED"
      },
      "source": [
        "This code fine-tunes a LLaMA 2 model on a medical dataset using LoRA (Low-Rank Adaptation) for efficient training. It first loads and tokenizes the dataset, then applies LoRA-based parameter-efficient fine-tuning to reduce GPU memory usage. The training configuration optimizes performance with gradient accumulation, mixed-precision (fp16), and logging via Weights & Biases (wandb). Finally, the SFTTrainer is initialized to handle the fine-tuning process efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS1Zpx1SuhBk"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# ✅ Load and preprocess the dataset\n",
        "train_dataset = load_dataset(\"aboonaji/wiki_medical_terms_llam2_format\", split=\"train\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return llama_tokenizer(\n",
        "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512\n",
        "    )\n",
        "\n",
        "# ✅ Apply tokenization and batching\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=4)\n",
        "\n",
        "# ✅ Define LoRA Config (Fixing the missing variable)\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\", r=64, lora_alpha=16, lora_dropout=0.1\n",
        ")\n",
        "\n",
        "# ✅ Define Training Arguments with Fewer Steps\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,  # ✅ Reduce epochs further if needed\n",
        "    max_steps=30,  # ✅ Stop training after 500 steps (Adjust as needed)\n",
        "    per_device_train_batch_size=1,  # ✅ Lower batch size to avoid OOM\n",
        "    gradient_accumulation_steps=8,  # ✅ Simulates larger batch\n",
        "    fp16=True,  # ✅ Mixed precision training\n",
        "    save_steps=100,  # ✅ Save model every 100 steps\n",
        "    save_total_limit=2,\n",
        "    logging_steps=10,\n",
        "    report_to=\"wandb\"  # ✅ Logs to Weights & Biases\n",
        ")\n",
        "\n",
        "# ✅ Initialize Trainer\n",
        "llama_sft_trainer = SFTTrainer(\n",
        "    model=llama_model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config  # ✅ Now defined correctly\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSF8SHFKt1xL"
      },
      "source": [
        "## Step 6: Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NlAIGjm0IxT"
      },
      "source": [
        "fdd17058c6cc236455c7169f3ebaebfc2628d1aa\n",
        "\n",
        "\n",
        "Copy the above key if prompted.\n",
        "Weights and Biases assists in hyperparameter tuning and debugging.\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jip3-ewIKZ1q",
        "outputId": "975ce123-f5be-4c1c-a4dd-b8c056e4dd42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "llama_sft_trainer.train()\n",
        "\n",
        "\"\"\"\n",
        "Weights & Biases (W&B) is integrated into the training process to track, visualize, and log key training metrics in real time. Since fine-tuning LLaMA 2 with LoRA involves multiple hyperparameters (batch size, gradient accumulation, fp16 training, etc.), W&B helps by:\n",
        "\n",
        "Logging Training Progress – Tracks loss, learning rate, and gradients to monitor model improvements.\n",
        "Experiment Tracking – Stores different runs, allowing easy comparison of training results.\n",
        "Reproducibility – Saves configurations and logs, making it easy to replicate experiments.\n",
        "Remote Monitoring – Provides a dashboard to visualize training even when running remotely (e.g., on Colab or a cloud GPU).\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhrRBaUAQdGY"
      },
      "outputs": [],
      "source": [
        "!pip install gtts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gF3n3RKJW30"
      },
      "outputs": [],
      "source": [
        "!pip install gtts playsound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKdVqxvGNS8e"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vUpnqWWgzZj"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install SpeechRecognition\n",
        "\n",
        "import speech_recognition as sr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V5u5akCtSfh"
      },
      "source": [
        "##Speech-to-Text Conversion Using Google Speech Recognition\n",
        "\n",
        "This code allows users to upload an audio file, process it, and convert speech into text using Google's Speech Recognition API. It first uploads an audio file in Google Colab, extracts the filename, and initializes a speech recognizer. The audio is then processed, and speech is transcribed into text. If the recognition fails due to unclear audio or API issues, appropriate error messages are displayed. This is useful for voice assistants, transcription services, and NLP applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUtH16whYEFk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhaZhe4XJ7IR"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install SpeechRecognition pyttsx3 pydub\n",
        "!apt-get install -y ffmpeg espeak  # Install eSpeak for pyttsx3\n",
        "\n",
        "# Import libraries\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "from IPython.display import Javascript, display, Audio\n",
        "from google.colab import output\n",
        "import base64\n",
        "import io\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# JavaScript to record audio in WAV format\n",
        "RECORD_JS = \"\"\"\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })  // Record in webm\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async () => {\n",
        "    blob = new Blob(chunks, { type: 'audio/webm' })\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "# Global list to store recognized texts\n",
        "recognized_texts = ['entry text']\n",
        "\n",
        "def record_audio(seconds=10):\n",
        "    \"\"\"Starts audio recording and waits for result.\"\"\"\n",
        "    display(Javascript(RECORD_JS))\n",
        "    audio_data = output.eval_js(f\"record({seconds * 1000})\")  # Wait for recording to finish\n",
        "    return process_audio(audio_data)\n",
        "\n",
        "def process_audio(audio_data):\n",
        "    \"\"\"Processes the recorded audio and converts speech to text.\"\"\"\n",
        "    global recognized_texts\n",
        "\n",
        "    try:\n",
        "        # Decode base64 audio\n",
        "        if \",\" in audio_data:\n",
        "            audio_bytes = base64.b64decode(audio_data.split(\",\")[1])\n",
        "        else:\n",
        "            print(\"Invalid audio data received.\")\n",
        "            return None\n",
        "\n",
        "        # Convert webm to WAV using pydub\n",
        "        webm_audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format=\"webm\")\n",
        "        wav_audio = webm_audio.set_frame_rate(16000).set_channels(1)\n",
        "        wav_audio.export(\"recorded_audio.wav\", format=\"wav\")\n",
        "        print(\"Audio converted to WAV format\")\n",
        "\n",
        "        # Recognize speech\n",
        "        recognizer = sr.Recognizer()\n",
        "        with sr.AudioFile(\"recorded_audio.wav\") as source:\n",
        "            audio = recognizer.record(source)\n",
        "            text = recognizer.recognize_google(audio)\n",
        "            recognized_texts.append(text)  # Append to list\n",
        "            print(\"You said:\", text)\n",
        "            SpeakText(text)\n",
        "            return text\n",
        "\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand audio\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Google API error: {e}\")\n",
        "\n",
        "    return None  # Return None if recognition fails\n",
        "\n",
        "def SpeakText(command):\n",
        "    \"\"\"Converts text to speech.\"\"\"\n",
        "    engine = pyttsx3.init()\n",
        "    engine.say(command)\n",
        "    engine.runAndWait()\n",
        "\n",
        "# Start first recording\n",
        "\"\"\"print(\"Recording... Speak now!\")\n",
        "text = record_audio(seconds=10)\n",
        "\n",
        "# Print all stored texts\n",
        "print(\"All Recorded Texts:\", recognized_texts)\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUdpbTAbJ7v_"
      },
      "outputs": [],
      "source": [
        "#print(\"Stored Text:\", recognized_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_x3M9blARfV"
      },
      "outputs": [],
      "source": [
        "\"\"\"print(\"Recording again...\")\n",
        "record_audio(seconds=10)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0GKa-bpASX6"
      },
      "outputs": [],
      "source": [
        "#print(\"All Recorded Texts:\", recognized_texts[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErN5p5ahvHI5"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install SpeechRecognition pyttsx3\n",
        "\n",
        "# Import libraries\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "from IPython.display import Javascript, display\n",
        "from google.colab import output\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V2h2lrevK_e"
      },
      "outputs": [],
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK00IQ4AvMn2"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "def translate_to_tamil(text):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, dest='ta').text\n",
        "    return translated_text\n",
        "\n",
        "# Example usage\n",
        "english_text = \"Hello, how are you?\"\n",
        "tamil_translation = translate_to_tamil(english_text)\n",
        "print(\"English:\", english_text)\n",
        "print(\"Tamil:\", tamil_translation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHdqpfv8vVV-"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install SpeechRecognition pyttsx3 pydub\n",
        "!apt-get install -y ffmpeg espeak  # Install eSpeak for pyttsx3\n",
        "\n",
        "# Import libraries\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "from IPython.display import Javascript, display, Audio\n",
        "from google.colab import output\n",
        "import base64\n",
        "import io\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMPw6WU6vbjP"
      },
      "source": [
        "## Chatting with the model\n",
        "\n",
        "This code implements a voice-based AI assistant named F.E.R.B, which interacts with users via text and speech. It listens for the activation phrase \"Hello\" and responds using Google Text-to-Speech (gTTS). If the user inputs a query, it processes the text using a LLaMA 2 text generation model and provides a response. The generated text is then converted into speech and played back. The assistant continues running until the user says \"Thank You\", at which point it exits. This setup creates an interactive AI chatbot with voice output, making it useful for virtual assistants, automation, or AI-driven conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hEG2z1CUKMV"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "import IPython.display as ipd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Assuming you have your model and tokenizer initialized\n",
        "# llama_model and llama_tokenizer should be defined already\n",
        "# Example:\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# llama_model = AutoModelForCausalLM.from_pretrained(\"your-llama-model\")\n",
        "# llama_tokenizer = AutoTokenizer.from_pretrained(\"your-llama-model\")\n",
        "\n",
        "State = True\n",
        "print(\"Recording again...\")\n",
        "record_audio(seconds=5)\n",
        "while State:\n",
        "    if recognized_texts[-1].lower() == 'hello':\n",
        "        print(\"Say 'Thank You' to exit.\")\n",
        "        intro = \"F.E.R.B: Hi there! How may I help you?\"\n",
        "        tts = gTTS(text=intro, lang='en')\n",
        "        tts.save(\"intro.mp3\")\n",
        "        ipd.display(ipd.Audio(\"intro.mp3\"))\n",
        "        print(intro)\n",
        "        tamil_translation = translate_to_tamil(intro)\n",
        "        print(tamil_translation)\n",
        "\n",
        "        while True:\n",
        "            print(\"Recording again...\")\n",
        "            record_audio(seconds=10)\n",
        "            if recognized_texts[-1].lower() == \"thank you\":\n",
        "                outro = \"F.E.R.B: Bye!\"\n",
        "                tts = gTTS(text=outro, lang='en')\n",
        "                tts.save(\"outro.mp3\")\n",
        "                ipd.display(ipd.Audio(\"outro.mp3\"))\n",
        "                print(outro)\n",
        "                tamil_translation = translate_to_tamil(outro)\n",
        "                State = False\n",
        "                break\n",
        "            else:\n",
        "                text_generation_pipeline = pipeline(task=\"text-generation\", model=llama_model, tokenizer=llama_tokenizer, max_length=300)\n",
        "                model_answer = text_generation_pipeline(f\"<s>[INST] {recognized_texts[-1]} [/INST]\")\n",
        "\n",
        "                # Remove <s>[INST] and [/INST] tags\n",
        "                generated_text = model_answer[0]['generated_text']\n",
        "                cleaned_text = generated_text.replace('<s>[INST]', '').replace('[/INST]', '').strip()\n",
        "\n",
        "                print('F.E.R.B:', cleaned_text)\n",
        "                tamil_text = translate_to_tamil(cleaned_text)\n",
        "                print(tamil_text)\n",
        "                tts = gTTS(text=cleaned_text, lang='en')\n",
        "                tts.save(\"answer.mp3\")\n",
        "                ipd.display(ipd.Audio(\"answer.mp3\"))\n",
        "\n",
        "    else:\n",
        "        print(\"Say 'Hello' to start the model.\")\n",
        "        State = False\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}